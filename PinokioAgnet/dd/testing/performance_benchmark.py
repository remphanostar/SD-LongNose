#!/usr/bin/env python3
"""
PinokioCloud Performance Benchmark - Phase 10

This module provides comprehensive performance benchmarking and optimization
validation for the entire PinokioCloud system.

Author: PinokioCloud Development Team
Version: 1.0.0
"""

import os
import sys
import json
import time
import psutil
import threading
import subprocess
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from pathlib import Path
from datetime import datetime, timedelta
import concurrent.futures

# Add the github_repo directory to Python path for imports
sys.path.append('/workspace/SD-LongNose/github_repo')

# Import optimization and monitoring components
from optimization.performance_monitor import PerformanceMonitor
from optimization.cache_manager import CacheManager
from cloud_detection.resource_assessor import ResourceAssessor


@dataclass
class BenchmarkMetric:
    """Represents a performance benchmark metric."""
    name: str
    value: float
    unit: str
    baseline: Optional[float] = None
    target: Optional[float] = None
    category: str = "general"
    description: str = ""


@dataclass
class BenchmarkResult:
    """Result of a performance benchmark test."""
    test_name: str
    metrics: List[BenchmarkMetric]
    success: bool
    duration: float
    peak_memory: float
    peak_cpu: float
    error_message: Optional[str] = None
    timestamp: datetime = field(default_factory=datetime.now)


class PerformanceBenchmark:
    """
    Comprehensive performance benchmarking system for PinokioCloud.
    
    Measures system performance across all components and validates
    that performance targets are met for production readiness.
    """
    
    def __init__(self, base_path: str = "/workspace/SD-LongNose"):
        """Initialize the performance benchmark system."""
        self.base_path = Path(base_path)
        self.github_repo_path = self.base_path / "github_repo"
        self.benchmark_results: List[BenchmarkResult] = []
        
        # Initialize monitoring components
        self.performance_monitor = PerformanceMonitor()
        self.cache_manager = CacheManager()
        self.resource_assessor = ResourceAssessor()
        
        # System information
        self.system_info = self.get_system_info()
        
        # Performance targets (can be adjusted based on platform)
        self.performance_targets = {
            'response_time': 2.0,  # seconds
            'memory_usage': 2048,  # MB
            'cpu_usage': 80.0,     # percentage
            'startup_time': 30.0,  # seconds
            'file_io_speed': 10.0, # MB/s
            'cache_hit_rate': 80.0, # percentage
        }
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get comprehensive system information."""
        try:
            return {
                'cpu_count': psutil.cpu.cores_logical(),
                'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {},
                'memory_total': psutil.virtual_memory().total,
                'disk_total': psutil.disk_usage('/').total,
                'python_version': sys.version,
                'platform': os.uname()._asdict()
            }
        except Exception as e:
            return {'error': str(e)}
    
    def monitor_resource_usage(self, duration: float = 1.0) -> Dict[str, float]:
        """Monitor resource usage for a specified duration."""
        start_time = time.time()
        cpu_samples = []
        memory_samples = []
        
        while time.time() - start_time < duration:
            cpu_samples.append(psutil.cpu_percent(interval=0.1))
            memory_samples.append(psutil.virtual_memory().percent)
            time.sleep(0.1)
        
        return {
            'peak_cpu': max(cpu_samples) if cpu_samples else 0.0,
            'avg_cpu': sum(cpu_samples) / len(cpu_samples) if cpu_samples else 0.0,
            'peak_memory': max(memory_samples) if memory_samples else 0.0,
            'avg_memory': sum(memory_samples) / len(memory_samples) if memory_samples else 0.0
        }
    
    def benchmark_system_startup(self) -> BenchmarkResult:
        """Benchmark system startup performance."""
        print("üöÄ Benchmarking system startup performance...")
        
        start_time = time.time()
        metrics = []
        
        try:
            # Monitor resource usage during startup simulation
            resource_monitor = threading.Thread(
                target=lambda: self.monitor_resource_usage(10.0)
            )
            resource_monitor.start()
            
            # Simulate system initialization
            startup_start = time.time()
            
            # Initialize core components
            component_times = {}
            
            # Cloud detection
            comp_start = time.time()
            from cloud_detection.cloud_detector import CloudDetector
            detector = CloudDetector()
            detection_result = detector.detect_cloud_platform()
            component_times['cloud_detection'] = time.time() - comp_start
            
            # Environment management
            comp_start = time.time()
            from environment_management.venv_manager import VirtualEnvironmentManager
            venv_manager = VirtualEnvironmentManager()
            component_times['environment_management'] = time.time() - comp_start
            
            # App analysis
            comp_start = time.time()
            from app_analysis.app_analyzer import AppAnalyzer
            app_analyzer = AppAnalyzer()
            component_times['app_analysis'] = time.time() - comp_start
            
            total_startup_time = time.time() - startup_start
            
            # Create metrics
            metrics.append(BenchmarkMetric(
                name="total_startup_time",
                value=total_startup_time,
                unit="seconds",
                target=self.performance_targets['startup_time'],
                category="startup",
                description="Total time to initialize all core components"
            ))
            
            for component, duration in component_times.items():
                metrics.append(BenchmarkMetric(
                    name=f"{component}_startup_time",
                    value=duration,
                    unit="seconds",
                    category="startup",
                    description=f"Startup time for {component} component"
                ))
            
            # Resource usage
            resource_usage = self.monitor_resource_usage(1.0)
            metrics.append(BenchmarkMetric(
                name="startup_peak_cpu",
                value=resource_usage['peak_cpu'],
                unit="percent",
                target=self.performance_targets['cpu_usage'],
                category="resources",
                description="Peak CPU usage during startup"
            ))
            
            metrics.append(BenchmarkMetric(
                name="startup_peak_memory",
                value=resource_usage['peak_memory'],
                unit="percent",
                category="resources",
                description="Peak memory usage during startup"
            ))
            
            total_duration = time.time() - start_time
            success = total_startup_time <= self.performance_targets['startup_time']
            
            return BenchmarkResult(
                test_name="system_startup",
                metrics=metrics,
                success=success,
                duration=total_duration,
                peak_memory=resource_usage['peak_memory'],
                peak_cpu=resource_usage['peak_cpu']
            )
            
        except Exception as e:
            return BenchmarkResult(
                test_name="system_startup",
                metrics=metrics,
                success=False,
                duration=time.time() - start_time,
                peak_memory=0.0,
                peak_cpu=0.0,
                error_message=str(e)
            )
    
    def benchmark_file_operations(self) -> BenchmarkResult:
        """Benchmark file I/O performance."""
        print("üìÅ Benchmarking file operations performance...")
        
        start_time = time.time()
        metrics = []
        
        try:
            import tempfile
            import shutil
            
            test_dir = tempfile.mkdtemp(prefix="pinokio_perf_test_")
            test_data_size = 10 * 1024 * 1024  # 10MB
            test_data = b"x" * test_data_size
            
            # Write performance test
            write_start = time.time()
            test_files = []
            for i in range(5):  # Write 50MB total
                test_file = os.path.join(test_dir, f"test_file_{i}.dat")
                with open(test_file, 'wb') as f:
                    f.write(test_data)
                test_files.append(test_file)
            write_duration = time.time() - write_start
            write_speed = (50.0) / write_duration  # MB/s
            
            # Read performance test
            read_start = time.time()
            for test_file in test_files:
                with open(test_file, 'rb') as f:
                    _ = f.read()
            read_duration = time.time() - read_start
            read_speed = (50.0) / read_duration  # MB/s
            
            # Random access test
            random_start = time.time()
            for test_file in test_files[:3]:
                with open(test_file, 'r+b') as f:
                    f.seek(test_data_size // 2)
                    f.write(b"random_access_test")
            random_duration = time.time() - random_start
            
            # Directory operations test
            dir_start = time.time()
            sub_dirs = []
            for i in range(100):
                sub_dir = os.path.join(test_dir, f"subdir_{i}")
                os.makedirs(sub_dir)
                sub_dirs.append(sub_dir)
            dir_duration = time.time() - dir_start
            
            # Cleanup
            shutil.rmtree(test_dir, ignore_errors=True)
            
            # Create metrics
            metrics.append(BenchmarkMetric(
                name="file_write_speed",
                value=write_speed,
                unit="MB/s",
                target=self.performance_targets['file_io_speed'],
                category="file_io",
                description="Sequential file write speed"
            ))
            
            metrics.append(BenchmarkMetric(
                name="file_read_speed",
                value=read_speed,
                unit="MB/s",
                target=self.performance_targets['file_io_speed'],
                category="file_io", 
                description="Sequential file read speed"
            ))
            
            metrics.append(BenchmarkMetric(
                name="random_access_time",
                value=random_duration,
                unit="seconds",
                category="file_io",
                description="Random file access performance"
            ))
            
            metrics.append(BenchmarkMetric(
                name="directory_creation_rate",
                value=100.0 / dir_duration,
                unit="dirs/s",
                category="file_io",
                description="Directory creation rate"
            ))
            
            total_duration = time.time() - start_time
            success = (write_speed >= self.performance_targets['file_io_speed'] and 
                      read_speed >= self.performance_targets['file_io_speed'])
            
            resource_usage = self.monitor_resource_usage(1.0)
            
            return BenchmarkResult(
                test_name="file_operations",
                metrics=metrics,
                success=success,
                duration=total_duration,
                peak_memory=resource_usage['peak_memory'],
                peak_cpu=resource_usage['peak_cpu']
            )
            
        except Exception as e:
            return BenchmarkResult(
                test_name="file_operations",
                metrics=metrics,
                success=False,
                duration=time.time() - start_time,
                peak_memory=0.0,
                peak_cpu=0.0,
                error_message=str(e)
            )
    
    def benchmark_cache_performance(self) -> BenchmarkResult:
        """Benchmark cache system performance."""
        print("üöÄ Benchmarking cache performance...")
        
        start_time = time.time()
        metrics = []
        
        try:
            # Test cache operations
            cache_operations = 1000
            test_data_size = 1024  # 1KB per cache entry
            test_data = b"x" * test_data_size
            
            # Cache write performance
            write_start = time.time()
            for i in range(cache_operations):
                self.cache_manager.put(
                    f"test_key_{i}", test_data, 
                    cache_type="memory", ttl=3600
                )
            write_duration = time.time() - write_start
            write_rate = cache_operations / write_duration
            
            # Cache read performance (should be fast due to memory cache)
            read_start = time.time()
            hit_count = 0
            for i in range(cache_operations):
                result = self.cache_manager.get(f"test_key_{i}")
                if result is not None:
                    hit_count += 1
            read_duration = time.time() - read_start
            read_rate = cache_operations / read_duration
            hit_rate = (hit_count / cache_operations) * 100
            
            # Cache statistics
            cache_stats = self.cache_manager.get_cache_statistics()
            
            # Create metrics
            metrics.append(BenchmarkMetric(
                name="cache_write_rate",
                value=write_rate,
                unit="ops/s",
                category="cache",
                description="Cache write operations per second"
            ))
            
            metrics.append(BenchmarkMetric(
                name="cache_read_rate",
                value=read_rate,
                unit="ops/s",
                category="cache",
                description="Cache read operations per second"
            ))
            
            metrics.append(BenchmarkMetric(
                name="cache_hit_rate",
                value=hit_rate,
                unit="percent",
                target=self.performance_targets['cache_hit_rate'],
                category="cache",
                description="Cache hit rate percentage"
            ))
            
            metrics.append(BenchmarkMetric(
                name="cache_memory_usage",
                value=cache_stats.get('memory_usage', 0),
                unit="MB",
                category="cache",
                description="Cache memory usage"
            ))
            
            total_duration = time.time() - start_time
            success = hit_rate >= self.performance_targets['cache_hit_rate']
            
            resource_usage = self.monitor_resource_usage(1.0)
            
            return BenchmarkResult(
                test_name="cache_performance",
                metrics=metrics,
                success=success,
                duration=total_duration,
                peak_memory=resource_usage['peak_memory'],
                peak_cpu=resource_usage['peak_cpu']
            )
            
        except Exception as e:
            return BenchmarkResult(
                test_name="cache_performance",
                metrics=metrics,
                success=False,
                duration=time.time() - start_time,
                peak_memory=0.0,
                peak_cpu=0.0,
                error_message=str(e)
            )
    
    def benchmark_concurrent_operations(self) -> BenchmarkResult:
        """Benchmark concurrent operations performance."""
        print("‚ö° Benchmarking concurrent operations performance...")
        
        start_time = time.time()
        metrics = []
        
        try:
            # Test concurrent file operations
            def concurrent_file_task(task_id: int) -> float:
                task_start = time.time()
                
                # Create temporary file
                import tempfile
                with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                    temp_path = temp_file.name
                    # Write data
                    test_data = f"Task {task_id} data " * 1000  # ~15KB
                    temp_file.write(test_data.encode())
                
                # Read data back
                with open(temp_path, 'r') as f:
                    _ = f.read()
                
                # Cleanup
                os.unlink(temp_path)
                
                return time.time() - task_start
            
            # Run concurrent tasks
            num_threads = min(10, os.cpu.cores_logical() or 4)
            concurrent_start = time.time()
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
                futures = [executor.submit(concurrent_file_task, i) for i in range(50)]
                task_durations = [future.result() for future in concurrent.futures.as_completed(futures)]
            
            concurrent_duration = time.time() - concurrent_start
            concurrent_throughput = 50 / concurrent_duration  # tasks/second
            
            # Test concurrent cache operations
            def concurrent_cache_task(task_id: int) -> float:
                task_start = time.time()
                
                # Cache operations
                key = f"concurrent_key_{task_id}"
                data = f"concurrent_data_{task_id}" * 100
                
                self.cache_manager.put(key, data, cache_type="memory")
                result = self.cache_manager.get(key)
                
                return time.time() - task_start
            
            cache_start = time.time()
            with concurrent.futures.ThreadPoolExecutor(max_workers=num_threads) as executor:
                futures = [executor.submit(concurrent_cache_task, i) for i in range(100)]
                cache_durations = [future.result() for future in concurrent.futures.as_completed(futures)]
            
            cache_concurrent_duration = time.time() - cache_start
            cache_throughput = 100 / cache_concurrent_duration  # ops/second
            
            # Create metrics
            metrics.append(BenchmarkMetric(
                name="concurrent_file_throughput",
                value=concurrent_throughput,
                unit="tasks/s",
                category="concurrency",
                description="Concurrent file operation throughput"
            ))
            
            metrics.append(BenchmarkMetric(
                name="avg_concurrent_task_duration",
                value=sum(task_durations) / len(task_durations),
                unit="seconds",
                category="concurrency",
                description="Average concurrent task duration"
            ))
            
            metrics.append(BenchmarkMetric(
                name="concurrent_cache_throughput",
                value=cache_throughput,
                unit="ops/s",
                category="concurrency",
                description="Concurrent cache operation throughput"
            ))
            
            metrics.append(BenchmarkMetric(
                name="thread_efficiency",
                value=num_threads / max(task_durations) * concurrent_duration,
                unit="ratio",
                category="concurrency",
                description="Thread utilization efficiency"
            ))
            
            total_duration = time.time() - start_time
            success = concurrent_throughput > 5.0  # At least 5 tasks/second
            
            resource_usage = self.monitor_resource_usage(2.0)
            
            return BenchmarkResult(
                test_name="concurrent_operations",
                metrics=metrics,
                success=success,
                duration=total_duration,
                peak_memory=resource_usage['peak_memory'],
                peak_cpu=resource_usage['peak_cpu']
            )
            
        except Exception as e:
            return BenchmarkResult(
                test_name="concurrent_operations",
                metrics=metrics,
                success=False,
                duration=time.time() - start_time,
                peak_memory=0.0,
                peak_cpu=0.0,
                error_message=str(e)
            )
    
    def benchmark_memory_efficiency(self) -> BenchmarkResult:
        """Benchmark memory usage efficiency."""
        print("üß† Benchmarking memory efficiency...")
        
        start_time = time.time()
        metrics = []
        
        try:
            import gc
            
            # Get initial memory state
            gc.collect()  # Force garbage collection
            initial_memory = psutil.virtual_memory().used
            
            # Memory allocation test
            large_data = []
            allocation_start = time.time()
            
            # Allocate memory in chunks
            for i in range(100):
                chunk = b"x" * (1024 * 1024)  # 1MB chunks
                large_data.append(chunk)
                
                if i % 10 == 0:  # Check memory every 10 allocations
                    current_memory = psutil.virtual_memory().used
                    allocated_memory = (current_memory - initial_memory) / (1024 * 1024)  # MB
            
            allocation_duration = time.time() - allocation_start
            peak_allocated = (psutil.virtual_memory().used - initial_memory) / (1024 * 1024)  # MB
            
            # Memory deallocation test
            deallocation_start = time.time()
            large_data.clear()
            gc.collect()
            deallocation_duration = time.time() - deallocation_start
            
            # Check memory recovery
            final_memory = psutil.virtual_memory().used
            memory_recovered = (psutil.virtual_memory().used - final_memory) / (1024 * 1024)  # MB
            memory_recovery_rate = (memory_recovered / peak_allocated) * 100 if peak_allocated > 0 else 100
            
            # Memory fragmentation test (estimate)
            fragmentation_start = time.time()
            small_allocations = []
            for i in range(1000):
                small_allocations.append(bytearray(1024))  # 1KB each
            fragmentation_duration = time.time() - fragmentation_start
            
            # Create metrics
            metrics.append(BenchmarkMetric(
                name="memory_allocation_rate",
                value=100.0 / allocation_duration,  # MB/s
                unit="MB/s",
                category="memory",
                description="Memory allocation rate"
            ))
            
            metrics.append(BenchmarkMetric(
                name="peak_memory_usage",
                value=peak_allocated,
                unit="MB",
                target=self.performance_targets['memory_usage'],
                category="memory",
                description="Peak memory usage during test"
            ))
            
            metrics.append(BenchmarkMetric(
                name="memory_recovery_rate",
                value=memory_recovery_rate,
                unit="percent",
                category="memory",
                description="Memory recovery after deallocation"
            ))
            
            metrics.append(BenchmarkMetric(
                name="small_allocation_rate",
                value=1000.0 / fragmentation_duration,
                unit="allocs/s",
                category="memory",
                description="Small allocation performance"
            ))
            
            total_duration = time.time() - start_time
            success = (peak_allocated <= self.performance_targets['memory_usage'] and 
                      memory_recovery_rate >= 80.0)
            
            resource_usage = self.monitor_resource_usage(1.0)
            
            return BenchmarkResult(
                test_name="memory_efficiency",
                metrics=metrics,
                success=success,
                duration=total_duration,
                peak_memory=resource_usage['peak_memory'],
                peak_cpu=resource_usage['peak_cpu']
            )
            
        except Exception as e:
            return BenchmarkResult(
                test_name="memory_efficiency",
                metrics=metrics,
                success=False,
                duration=time.time() - start_time,
                peak_memory=0.0,
                peak_cpu=0.0,
                error_message=str(e)
            )
    
    def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """Run comprehensive performance benchmark suite."""
        print("üéØ PinokioCloud Performance Benchmark - Phase 10")
        print("=" * 60)
        
        start_time = datetime.now()
        
        print("üíª System Information:")
        for key, value in self.system_info.items():
            if key != 'error':
                if isinstance(value, dict):
                    print(f"  {key}: {json.dumps(value, default=str)}")
                else:
                    print(f"  {key}: {value}")
        
        print("\nüéØ Performance Targets:")
        for target, value in self.performance_targets.items():
            print(f"  {target}: {value}")
        
        print("\nüöÄ Running Performance Benchmarks...")
        
        # Run all benchmark tests
        benchmark_tests = [
            self.benchmark_system_startup,
            self.benchmark_file_operations,
            self.benchmark_cache_performance,
            self.benchmark_concurrent_operations,
            self.benchmark_memory_efficiency,
        ]
        
        for benchmark_test in benchmark_tests:
            try:
                result = benchmark_test()
                self.benchmark_results.append(result)
                
                status = "‚úÖ PASS" if result.success else "‚ùå FAIL"
                print(f"  {status} - {result.test_name} ({result.duration:.1f}s)")
                
                if result.error_message:
                    print(f"    Error: {result.error_message}")
                    
            except Exception as e:
                print(f"  ‚ùå FAIL - {benchmark_test.__name__}: {e}")
        
        end_time = datetime.now()
        
        # Generate comprehensive report
        return self.generate_benchmark_report(start_time, end_time)
    
    def generate_benchmark_report(self, start_time: datetime, end_time: datetime) -> Dict[str, Any]:
        """Generate comprehensive benchmark report."""
        total_tests = len(self.benchmark_results)
        successful_tests = sum(1 for r in self.benchmark_results if r.success)
        success_rate = (successful_tests / total_tests) * 100 if total_tests > 0 else 0
        
        # Aggregate metrics by category
        metrics_by_category = {}
        for result in self.benchmark_results:
            for metric in result.metrics:
                category = metric.category
                if category not in metrics_by_category:
                    metrics_by_category[category] = []
                metrics_by_category[category].append(metric)
        
        # Calculate performance scores
        performance_scores = {}
        for category, metrics in metrics_by_category.items():
            scores = []
            for metric in metrics:
                if metric.target is not None:
                    if metric.unit in ['seconds', 'percent'] and metric.name.endswith('_time'):
                        # Lower is better for time metrics
                        score = min(100, (metric.target / max(metric.value, 0.01)) * 100)
                    else:
                        # Higher is better for most other metrics
                        score = min(100, (metric.value / metric.target) * 100)
                    scores.append(score)
            
            performance_scores[category] = sum(scores) / len(scores) if scores else 0
        
        overall_performance_score = sum(performance_scores.values()) / len(performance_scores) if performance_scores else 0
        
        report = {
            'benchmark_summary': {
                'total_tests': total_tests,
                'successful_tests': successful_tests,
                'failed_tests': total_tests - successful_tests,
                'success_rate': success_rate,
                'overall_performance_score': overall_performance_score,
                'test_duration': str(end_time - start_time),
                'timestamp': datetime.now().isoformat(),
                'system_info': self.system_info
            },
            'performance_scores': performance_scores,
            'performance_targets': self.performance_targets,
            'detailed_results': [
                {
                    'test_name': r.test_name,
                    'success': r.success,
                    'duration': r.duration,
                    'peak_memory': r.peak_memory,
                    'peak_cpu': r.peak_cpu,
                    'metrics': [
                        {
                            'name': m.name,
                            'value': m.value,
                            'unit': m.unit,
                            'target': m.target,
                            'category': m.category,
                            'description': m.description
                        }
                        for m in r.metrics
                    ],
                    'error_message': r.error_message
                }
                for r in self.benchmark_results
            ],
            'failed_tests': [
                {
                    'test_name': r.test_name,
                    'error_message': r.error_message,
                    'duration': r.duration
                }
                for r in self.benchmark_results if not r.success
            ],
            'recommendations': self.generate_performance_recommendations()
        }
        
        return report
    
    def generate_performance_recommendations(self) -> List[str]:
        """Generate performance optimization recommendations."""
        recommendations = []
        
        if not self.benchmark_results:
            return ["No benchmark results available for analysis."]
        
        total_tests = len(self.benchmark_results)
        successful_tests = sum(1 for r in self.benchmark_results if r.success)
        
        # Overall performance assessment
        if successful_tests == total_tests:
            recommendations.append("üéâ All performance benchmarks passed! System is optimally configured.")
        elif successful_tests >= total_tests * 0.8:
            recommendations.append("‚úÖ Good performance overall. Minor optimizations recommended.")
        elif successful_tests >= total_tests * 0.5:
            recommendations.append("‚ö†Ô∏è  Moderate performance issues. Optimization needed.")
        else:
            recommendations.append("‚ùå Significant performance issues detected. Major optimization required.")
        
        # Specific recommendations based on failed tests
        failed_tests = [r for r in self.benchmark_results if not r.success]
        for result in failed_tests:
            if result.test_name == "system_startup":
                recommendations.append("üöÄ Optimize system startup - consider lazy loading and component caching.")
            elif result.test_name == "file_operations":
                recommendations.append("üìÅ Improve file I/O performance - check disk speed and implement buffering.")
            elif result.test_name == "cache_performance":
                recommendations.append("üöÄ Optimize caching strategy - increase cache size or improve eviction policy.")
            elif result.test_name == "concurrent_operations":
                recommendations.append("‚ö° Enhance concurrency handling - optimize thread pool size and task distribution.")
            elif result.test_name == "memory_efficiency":
                recommendations.append("üß† Optimize memory usage - implement memory pooling and reduce allocations.")
        
        # Resource-specific recommendations
        high_cpu_tests = [r for r in self.benchmark_results if r.peak_cpu > 90]
        if high_cpu_tests:
            recommendations.append("üî• High CPU usage detected - optimize algorithms and reduce computational complexity.")
        
        high_memory_tests = [r for r in self.benchmark_results if r.peak_memory > 80]
        if high_memory_tests:
            recommendations.append("üß† High memory usage detected - implement memory optimization and garbage collection tuning.")
        
        return recommendations
    
    def save_benchmark_report(self, report: Dict[str, Any], filename: str = None):
        """Save benchmark report to file."""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_benchmark_report_{timestamp}.json"
        
        report_path = self.base_path / filename
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        print(f"üìä Benchmark report saved: {report_path}")


def main():
    """Main benchmarking function."""
    print("üéØ PinokioCloud Performance Benchmark - Phase 10")
    print("=" * 60)
    
    # Initialize benchmark system
    benchmark = PerformanceBenchmark()
    
    # Run comprehensive benchmarking
    report = benchmark.run_comprehensive_benchmark()
    
    # Print summary
    print("\nüìä PERFORMANCE BENCHMARK RESULTS")
    print("=" * 50)
    summary = report['benchmark_summary']
    print(f"Total Tests: {summary['total_tests']}")
    print(f"Successful: {summary['successful_tests']}")
    print(f"Failed: {summary['failed_tests']}")
    print(f"Success Rate: {summary['success_rate']:.1f}%")
    print(f"Overall Performance Score: {summary['overall_performance_score']:.1f}/100")
    print(f"Test Duration: {summary['test_duration']}")
    
    print("\nüìà Performance Scores by Category:")
    for category, score in report['performance_scores'].items():
        print(f"  {category}: {score:.1f}/100")
    
    if report['failed_tests']:
        print("\n‚ùå Failed Tests:")
        for test in report['failed_tests']:
            print(f"  - {test['test_name']}: {test['error_message']}")
    
    print("\nüí° Recommendations:")
    for rec in report['recommendations']:
        print(f"  {rec}")
    
    # Save report
    benchmark.save_benchmark_report(report)
    
    return report['benchmark_summary']['success_rate'] >= 80  # Return True if success rate >= 80%


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)